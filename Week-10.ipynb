{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f476ca3",
   "metadata": {},
   "source": [
    "# Week-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f6f02",
   "metadata": {},
   "source": [
    "# Aim:- Write a program to implement TF-IDF for any corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd477f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import nltk \n",
    "corpus = [\"This is the first document.\", \n",
    "          \"This document is the second document.\", \n",
    "          \"And this is the third one.\", \n",
    "          \"Is this the first document?\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8dff5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'the', 'first', 'document', '.'],\n",
       " ['this', 'document', 'is', 'the', 'second', 'document', '.'],\n",
       " ['and', 'this', 'is', 'the', 'third', 'one', '.'],\n",
       " ['is', 'this', 'the', 'first', 'document', '?']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the corpus \n",
    "tokenized_corpus = [nltk.word_tokenize(doc.lower()) for doc in corpus] \n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31701d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'is': 4, 'document': 3, '.': 3, 'this': 4, 'the': 4, 'first': 2, 'second': 1, 'third': 1, 'and': 1, 'one': 1, '?': 1}\n"
     ]
    }
   ],
   "source": [
    "# Calculate document frequency (df) for each term in the corpus \n",
    "df = {} \n",
    "for doc in tokenized_corpus: \n",
    "    for term in set(doc): \n",
    "        if term not in df: \n",
    "            df[term] = 1 \n",
    "        else: \n",
    "            df[term] += 1\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4faae394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "{'is': 0.0, 'document': 0.28768207245178085, '.': 0.28768207245178085, 'this': 0.0, 'the': 0.0, 'first': 0.6931471805599453, 'second': 1.3862943611198906, 'third': 1.3862943611198906, 'and': 1.3862943611198906, 'one': 1.3862943611198906, '?': 1.3862943611198906}\n"
     ]
    }
   ],
   "source": [
    "# Calculate inverse document frequency (idf) for each term in the corpus \n",
    "N = len(tokenized_corpus) \n",
    "print(N)\n",
    "idf = {} \n",
    "for term in df: \n",
    "    idf[term] = math.log(N / df[term])\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3737a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'this': 1, 'is': 1, 'the': 1, 'first': 1, 'document': 1, '.': 1}, 1: {'this': 1, 'document': 2, 'is': 1, 'the': 1, 'second': 1, '.': 1}, 2: {'and': 1, 'this': 1, 'is': 1, 'the': 1, 'third': 1, 'one': 1, '.': 1}, 3: {'is': 1, 'this': 1, 'the': 1, 'first': 1, 'document': 1, '?': 1}}\n"
     ]
    }
   ],
   "source": [
    "# Calculate term frequency (tf) for each term in each document \n",
    "tf = {} \n",
    "for i, doc in enumerate(tokenized_corpus): \n",
    "    tf[i] = {} \n",
    "    for term in doc: \n",
    "        if term not in tf[i]: \n",
    "            tf[i][term] = 1 \n",
    "        else: \n",
    "            tf[i][term] += 1\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10b8f5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'this': 0.0, 'is': 0.0, 'the': 0.0, 'first': 0.6931471805599453, 'document': 0.28768207245178085, '.': 0.28768207245178085}, 1: {'this': 0.0, 'document': 0.5753641449035617, 'is': 0.0, 'the': 0.0, 'second': 1.3862943611198906, '.': 0.28768207245178085}, 2: {'and': 1.3862943611198906, 'this': 0.0, 'is': 0.0, 'the': 0.0, 'third': 1.3862943611198906, 'one': 1.3862943611198906, '.': 0.28768207245178085}, 3: {'is': 0.0, 'this': 0.0, 'the': 0.0, 'first': 0.6931471805599453, 'document': 0.28768207245178085, '?': 1.3862943611198906}}\n"
     ]
    }
   ],
   "source": [
    "# Calculate TF-IDF score for each term in each document \n",
    "tfidf = {} \n",
    "for i, doc in enumerate(tokenized_corpus): \n",
    "    tfidf[i] = {} \n",
    "    for term in doc: \n",
    "        tfidf[i][term] = tf[i][term] * idf[term] \n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32050a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0:\n",
      " this: 0.0\n",
      " is: 0.0\n",
      " the: 0.0\n",
      " first: 0.6931471805599453\n",
      " document: 0.28768207245178085\n",
      " .: 0.28768207245178085\n",
      "Document 1:\n",
      " this: 0.0\n",
      " document: 0.5753641449035617\n",
      " is: 0.0\n",
      " the: 0.0\n",
      " second: 1.3862943611198906\n",
      " .: 0.28768207245178085\n",
      "Document 2:\n",
      " and: 1.3862943611198906\n",
      " this: 0.0\n",
      " is: 0.0\n",
      " the: 0.0\n",
      " third: 1.3862943611198906\n",
      " one: 1.3862943611198906\n",
      " .: 0.28768207245178085\n",
      "Document 3:\n",
      " is: 0.0\n",
      " this: 0.0\n",
      " the: 0.0\n",
      " first: 0.6931471805599453\n",
      " document: 0.28768207245178085\n",
      " ?: 1.3862943611198906\n"
     ]
    }
   ],
   "source": [
    "# Print the TF-IDF scores for each document \n",
    "for i, doc in enumerate(tokenized_corpus): \n",
    "    print(f\"Document {i}:\") \n",
    "    for term in tfidf[i]: \n",
    "        print(f\" {term}: {tfidf[i][term]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158a225a-7a39-450c-8320-41f626c4ed51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
